{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ClC0m-KG2X8r"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "sX3-E6Z02d7h"
      },
      "outputs": [],
      "source": [
        "class SeriesDecomp(nn.Module):\n",
        "    def __init__(self, kernel_size):\n",
        "        super(SeriesDecomp, self).__init__()\n",
        "\n",
        "        # keep the series length unchanged\n",
        "        self.avg_pool = nn.AvgPool1d(kernel_size=kernel_size, stride=1, padding=kernel_size//2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # match the expected input format for the average pooling layer.\n",
        "        x_t = self.avg_pool(x.permute(0, 2, 1)).permute(0, 2, 1)\n",
        "\n",
        "        # x_s is the seasonal and x_t is the trend-cyclical component\n",
        "        x_s = x - x_t\n",
        "        return x_s, x_t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HyjlW_7wBhKz"
      },
      "outputs": [],
      "source": [
        "class AutoCorrelation(nn.Module):\n",
        "    def __init__(self, d_model, h, c):\n",
        "        super(AutoCorrelation, self).__init__()\n",
        "\n",
        "        self.d_model = d_model # dimension of the hidden state.\n",
        "        self.h = h # number of attention heads.\n",
        "        self.c = c # hyper-parameter for selecting the top-k autocorrelations.\n",
        "\n",
        "    def forward(self, Q, K, V):\n",
        "        B, L, _ = Q.size() # batch size, sequence length, and hidden dimension.\n",
        "\n",
        "        # reshape and permute for multi-headed attention\n",
        "        Q = Q.view(B, L, self.h, self.d_model // self.h).permute(0, 2, 1, 3)\n",
        "        K = K.view(B, L, self.h, self.d_model // self.h).permute(0, 2, 1, 3)\n",
        "        V = V.view(B, L, self.h, self.d_model // self.h).permute(0, 2, 1, 3)\n",
        "\n",
        "        # applies FFT to Q and K along the sequence dimension.\n",
        "        Q = torch.fft.fft(Q, dim=2)\n",
        "        K = torch.fft.fft(K, dim=2)\n",
        "\n",
        "        # computes the autocorrelation using the inverse FFT.\n",
        "        Corr = torch.fft.ifft(Q * torch.conj(K), dim=2).real\n",
        "\n",
        "        # calculates the number of top-k autocorrelations to consider.\n",
        "        topk = int(self.c * torch.log(torch.tensor(L, dtype=torch.float32)))\n",
        "\n",
        "        # selects the top-k autocorrelations and their indices.\n",
        "        W_topk, I_topk = torch.topk(Corr, topk, dim=2)\n",
        "\n",
        "        # applies softmax to the top-k autocorrelations.\n",
        "        W_topk = F.softmax(W_topk, dim=2)\n",
        "\n",
        "        # creates an index tensor for aggregation.\n",
        "        Index = torch.arange(L).unsqueeze(0).unsqueeze(0).unsqueeze(0).repeat(B, self.h, 1, 1)\n",
        "\n",
        "        # repeats V to match the dimensions for aggregation.\n",
        "        V = V.repeat(1, 1, 2, 1)\n",
        "\n",
        "        # initialize the result tensor R.\n",
        "        R = torch.zeros_like(V)\n",
        "\n",
        "        # Aggregates the similar sub-series using the top-k autocorrelations.\n",
        "        for i in range(topk):\n",
        "            R += W_topk[:, :, i, :].unsqueeze(2) * V.gather(2, (I_topk[:, :, i, :].unsqueeze(2) + Index).clamp(max=L-1)) # clamping the index values to not exceed the length\n",
        "\n",
        "        # Sums the aggregated sub-series.\n",
        "        R = R.sum(dim=2)\n",
        "\n",
        "        # Permutes and reshapes R to match the original format.\n",
        "        return R.permute(0, 2, 1, 3).contiguous().view(B, L, self.d_model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "71jtPootBwfx"
      },
      "outputs": [],
      "source": [
        "class AutoformerEncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, h, c, kernel_size):\n",
        "        super(AutoformerEncoderLayer, self).__init__()\n",
        "\n",
        "        \"\"\"\n",
        "        d_model: The dimension of the hidden state.\n",
        "        h: The number of attention heads.\n",
        "        c: A hyper-parameter for selecting the top-k autocorrelations.\n",
        "        kernel_size: The size of the moving average window.\n",
        "        \"\"\"\n",
        "\n",
        "        self.series_decomp = SeriesDecomp(kernel_size)\n",
        "        self.auto_correlation = AutoCorrelation(d_model, h, c)\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(d_model, d_model),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model)\n",
        "        )\n",
        "\n",
        "    # algo 1: lines 5 to 8\n",
        "    def forward(self, x):\n",
        "        x_s,_ = self.series_decomp(self.auto_correlation(x, x, x) + x)\n",
        "\n",
        "        # MODIFIED: use series decomp after ffwd\n",
        "        x_s, _ = self.series_decomp(self.feed_forward(x_s) + x_s)\n",
        "\n",
        "        return x_s\n",
        "\n",
        "class AutoformerDecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, h, c, kernel_size):\n",
        "        super(AutoformerDecoderLayer, self).__init__()\n",
        "        self.series_decomp = SeriesDecomp(kernel_size)\n",
        "        self.auto_correlation = AutoCorrelation(d_model, h, c)\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(d_model, d_model),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model)\n",
        "        )\n",
        "\n",
        "        # MODIFIED: 3 separate MLPs\n",
        "        self.mlp1 = nn.Linear(d_model, d_model)\n",
        "        self.mlp2 = nn.Linear(d_model, d_model)\n",
        "        self.mlp3 = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, x, enc_output, x_t):\n",
        "        s1, t1 = self.series_decomp(self.auto_correlation(x, x, x) + x)\n",
        "        s2, t2 = self.series_decomp(self.auto_correlation(s1, enc_output, enc_output) + s1)\n",
        "        s3, t3 = self.series_decomp(self.feed_forward(s2) + s2)\n",
        "\n",
        "        t = x_t + self.mlp1(t1) + self.mlp2(t2) + self.mlp3(t3)\n",
        "\n",
        "        return s3, t # MODIFIED: outputs final seasonal and agg t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5gRjnGk6B0HB"
      },
      "outputs": [],
      "source": [
        "class Autoformer(nn.Module):\n",
        "    def __init__(self, d, d_model, h, c, kernel_size, N, M):\n",
        "        super(Autoformer, self).__init__()\n",
        "\n",
        "        \"\"\"\n",
        "        d_model: The dimension of the hidden state.\n",
        "        h: The number of attention heads.\n",
        "        c: A hyper-parameter for selecting the top-k autocorrelations.\n",
        "        kernel_size: The size of the moving average window.\n",
        "        N: The number of encoder layers.\n",
        "        M: The number of decoder layers.\n",
        "        \"\"\"\n",
        "        self.embed = nn.Linear(d, d_model) # linear layer to embed the input time series.\n",
        "        self.encoder_layers = nn.ModuleList([AutoformerEncoderLayer(d_model, h, c, kernel_size) for _ in range(N)])\n",
        "        self.decoder_layers = nn.ModuleList([AutoformerDecoderLayer(d_model, h, c, kernel_size) for _ in range(M)])\n",
        "        self.mlp = nn.Linear(d_model, d) # projects the hidden state to the output dimension.\n",
        "\n",
        "        self.N = N\n",
        "        self.series_decomp = SeriesDecomp(kernel_size)\n",
        "\n",
        "    def forward(self, X, I, O):\n",
        "\n",
        "        B, _, d = X.shape\n",
        "\n",
        "        # decompose the embedded input into seasonal and trend-cyclical components.\n",
        "        X_en_s, X_en_t = self.series_decomp(X[I//2:])\n",
        "\n",
        "        # Encoder\n",
        "        for layer in self.encoder_layers:\n",
        "            X_en_s = layer(X_en_s)\n",
        "\n",
        "        # Prepare decoder input\n",
        "        X_de_s = torch.cat([X_en_s, torch.zeros(B, O, d, device=X.device)], dim=1)\n",
        "        X_de_t = torch.cat([X_en_t, X_en.mean(dim=1, keepdim=True).repeat(1, O, 1)], dim=1)\n",
        "\n",
        "        # Decoder\n",
        "        for layer in self.decoder_layers:\n",
        "            X_de_s, X_de_t = layer(X_de_s, X_en_s)\n",
        "\n",
        "        # Final prediction\n",
        "        X_pred = self.mlp(X_de_s) + X_de_t\n",
        "\n",
        "        return X_pred[:, -O:, :]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vy4bZYulCTqz"
      },
      "outputs": [],
      "source": [
        "# Parameters\n",
        "d = 10          # example input dim\n",
        "d_model = 512   # dimension of the hidden state\n",
        "h = 8           # number of attention heads\n",
        "c = 2           # hyper-parameter for selecting the top-k autocorrelations\n",
        "kernel_size = 25\n",
        "N = 2\n",
        "M = 1\n",
        "d = 10  # Example input dimension\n",
        "\n",
        "# Create model\n",
        "model = Autoformer(d, d_model, h, c, kernel_size, N, M)\n",
        "\n",
        "# Example input\n",
        "X = torch.randn(I, d)  # I is the input length, d is the input dimension\n",
        "\n",
        "# Forward pass\n",
        "output = model(X, I, O)  # O is the prediction length\n",
        "\n",
        "print(output)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
